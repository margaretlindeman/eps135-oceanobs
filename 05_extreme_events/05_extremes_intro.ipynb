{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5bb08c-0063-48a9-ba47-ce7661bb0040",
   "metadata": {},
   "source": [
    "## EPS/ESE 135: Observing the Ocean\n",
    "### Data Analysis Assignment 5 (Intro): Extreme events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54caba0-7fb7-4f91-801c-1d3c3737c198",
   "metadata": {},
   "source": [
    "*You don't have to run any code in this notebook, but I have provided suggested Python approaches to the exercises, so please read to the end.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86257e9c-607a-4b10-bdf5-358ca3fa69bc",
   "metadata": {},
   "source": [
    "For this assignment you will be analyzing a tide gauge record from Newport, Rhode Island. This record comes from a [University of Hawaii Sea Level Center (UHSLC) database](https://uhslc.soest.hawaii.edu/data/) that archives hourly and daily data for certain tide gauge stations. In contrast to the data from PSMSL that you worked with last week, which were averaged monthly, the higher temporal resolution of the UHSLC data will allow us to look at individual extreme events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06eb0cf-2d45-470e-9441-21f92598087c",
   "metadata": {},
   "source": [
    "This assignment is based on the first two tutorials of the [ClimateMatch Academy](https://comptools.climatematch.io/tutorials/intro.html) materials on [Extremes and Variability](https://comptools.climatematch.io/tutorials/W2D3_ExtremesandVariability/student/W2D3_Intro.html). If you are looking for more details or background information take a look at the materials on their website, which is a really great resource for climate data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431831a-0e47-456e-b951-5560482c1ab6",
   "metadata": {},
   "source": [
    "### Defining an extreme event\n",
    "\n",
    "When you plot the Newport sea level record, you'll see that there is a huge amount of variability (including some pretty large tides). You can quickly use pandas to generate a list of summary statistics for a dataframe `df` using the function `df.describe()`.\n",
    "\n",
    "However, if we are interested in high sea level events, we can first extract the maximum recorded sea level for each year and then look at the statistics of those extreme values.\n",
    "\n",
    "This will allow us to compute **return periods**, or the expected number of years between events of a certain magnitude. This is one way that scientists and engineers define e.g. a 50-year flood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116783bf-4972-4401-8444-0e794edb8885",
   "metadata": {},
   "source": [
    "### Calculating return periods\n",
    "\n",
    "Start by sorting the annual maxima from highest to lowest. These will be called the **return levels** (i.e. the magnitudes of the extreme events). Then assign each one a rank $r$, with $r=1$ corresponding to the highest value and $r=N$ to the lowest value.\n",
    "\n",
    "The probability of exceeding any value in a given year $P$ can be calculated as $P=r/(N+1)$.\n",
    "\n",
    "The corresponding return period $T$ is the inverse of the exceedance probability, $T = 1/P$. (In other words, the lower the probability of an event, the higher the predicted number of years between events of that magnitude.)\n",
    "\n",
    "Clearly, this does **not** mean that it's impossible for an event to occur more (or less) frequently than its predicted return period! It just means that, based on this record, this is one estimate of the probability of such an event happening in a given year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a47aab-9d4d-42a9-a43c-397ddb37a32e",
   "metadata": {},
   "source": [
    "### Python tools for this assignment\n",
    "\n",
    "There is some overlap with the last assignment, with a few new tricks thrown in. My goal here is to show you all of the additional tools you will need to do this assignment but you should implement them yourself! Give your variables sensible names and add your own comments, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc8d77-3258-43fe-bb22-30a8d01c4de1",
   "metadata": {},
   "source": [
    "As a general rule, you should always be importing all of the packages you need to run your scripts at the top; there's no need to import them again in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632296e-fe33-46d4-8553-1745e85c3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python modules for assignment 5\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694ba9c-c5f9-44be-9b0e-aaccc7e04b9a",
   "metadata": {},
   "source": [
    "The csv file has 5 columns: year, month, day, hour, and relative sea level in millimeters. There are no headers so you should add them manually when you read the csv into pandas.\n",
    "\n",
    "![screenshot of csv file](csvscreenshot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f6e58-ce02-416f-8245-dc539df64edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you decide what to name your dataframe and insert the correct filename\n",
    "df0 = pd.read_csv(filename,names=['year','month','day','hour','SL'])\n",
    "\n",
    "# the fill value in this dataset is -32767 for some reason so let's cut\n",
    "#    those out:\n",
    "df1 = df0[df0['SL']>-30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cc421-2df3-4f11-bc3e-0fc4c4d693cf",
   "metadata": {},
   "source": [
    "As we've run into a few times, different ways of expressing dates/time can have different benefits. In this assignment, you will use both: datetime because it makes it easy to group data with pandas, and decimal years because they are convenient for calculating trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f31ebd-4110-49e3-ac17-545e81c7d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with the datetime \n",
    "df1['datetime'] = pd.to_datetime(df1[[\"year\", \"month\", \"day\", \"hour\"]])\n",
    "df1.set_index('datetime') # tells pandas to use datetime as the index\n",
    "\n",
    "# now use datetime to calculate decimal years:\n",
    "# total_seconds is a built-in function that will calculate seconds elapsed\n",
    "#     between datetime values\n",
    "x_sec = (df1.index - df1.index[0]).total_seconds()\n",
    "\n",
    "# how many seconds per year?\n",
    "# 365.25 days/year\n",
    "# 24 hours/day...\n",
    "seconds_per_year = ...\n",
    "\n",
    "# calculate decimal years\n",
    "dec_year = x_sec/seconds_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581ebc0-2d44-473d-8211-c26fdf06d5ca",
   "metadata": {},
   "source": [
    "Note that this gives you decimal years starting from 0 (rather than from the middle of 1930); you will use this to calculate a linear trend so it's okay for this purpose if they are offset.\n",
    "\n",
    "Since your dataframe is indexed by datetime, you can simply use `resample` to get the annual maxima that you will use for the statistical calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb751eb-6ccb-4dc0-b466-9bb881789716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please name this dataframe something descriptive\n",
    "df2 = df1.resample(\"YE\").max()\n",
    "\n",
    "# at this point you could also clean things up by dropping extra columns\n",
    "df2 = df2[[\"SL\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2cb97c-8205-4e60-8a9b-7a9a41863d14",
   "metadata": {},
   "source": [
    "You can easily quickly plot a histogram of any dataframe using `df['variable'].hist()` and you can add a vertical line to a plot using `plt.axvline(x)`. You can add a `label='description'` argument to each of those if you are going to add a legend to your plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aac635-4c5e-4a71-bb37-41ee292590eb",
   "metadata": {},
   "source": [
    "**Calculating return periods**\n",
    "\n",
    "If there are `nan` (not-a-number) values in your dataframe, Python doesn't know how to rank those, so you need to remove them first.\n",
    "\n",
    "Use [np.sort()](https://numpy.org/doc/stable/reference/generated/numpy.sort.html) to sort the data. This will sort in ascending order (lowest to highest), so you can reverse the order using an [indexing trick](https://www.geeksforgeeks.org/python/how-to-reverse-a-list-in-python-using-slicing/). \n",
    "\n",
    "Then use [stats.rankdata()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.rankdata.html) to rank the maxima. Again, by default this function ranks values from lowest to highest. Here you can flip the order by multiplying by negative 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c477b581-85f9-40cd-bf2a-0f13b5c95f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows from dataframe\n",
    "df3 =  df2.dropna()\n",
    "\n",
    "# create a numpy array of return levels sorted from highest to lowest\n",
    "np1 = np.sort(df3['SL'])[::-1]\n",
    "\n",
    "# does it look right?\n",
    "np1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ad3d2-9885-4b1c-8afa-399a04cf2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scipy to rank the return levels\n",
    "np2 = stats.rankdata(np1*-1)\n",
    "\n",
    "# does it look right? \n",
    "# (rank 1 should correspond to the highest value of the sorted list and\n",
    "#    if there are any duplicate values in the sorted list they should have\n",
    "#    the same rank here)\n",
    "np2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542dae6-336e-4c63-805e-9850b7a654b6",
   "metadata": {},
   "source": [
    "Now you can do the calculations given above for exceedance probability $P$ and return period $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde15d11-b9b9-49c6-abb1-35c04083d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of years of data\n",
    "N = len(np1)\n",
    "\n",
    "# calculation for exceedance probability\n",
    "P = ...\n",
    "\n",
    "# calculation for return period \n",
    "T = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287b305-5c4c-49dc-bdf8-e15c93b6d5c9",
   "metadata": {},
   "source": [
    "To make a scatter plot with a log scale on the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21742079-5848-40fc-932c-db10189c5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create axis object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot data on axis\n",
    "ax.plot(x_variable, y_variable, \"o\")\n",
    "\n",
    "# choose x-axis scale:\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38068e1c-f74b-4bb0-94ff-c82e794468cb",
   "metadata": {},
   "source": [
    "Keep adding appropriate axis labels, titles, and line labels/legends to your plots!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
